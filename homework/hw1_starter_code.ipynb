{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import aa598.hw1_helper as hw1_helper\n",
    "\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=True) # set to False if latex is not set up on your computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/hw1/wave_data_train.pickle\", 'rb') as handle:\n",
    "    wave_data = pickle.load(handle)\n",
    "history = wave_data[\"history\"]\n",
    "future = wave_data[\"future\"]\n",
    "\n",
    "index = 1\n",
    "history_length = history.shape[-1]\n",
    "future_length = future.shape[-1]\n",
    "ts_history = np.arange(-history_length,0)\n",
    "ts_future = np.arange(future_length)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(ts_history, history[index], marker='o', linestyle='--', label=\"History\")\n",
    "plt.plot([-1,0], [history[index][-1], future[index][0]], marker='o', linestyle='--', color=\"C0\")\n",
    "plt.plot(ts_future, future[index], markersize=7, marker='o', linestyle='--', label=\"Future\")\n",
    "\n",
    "plt.xlabel('Steps', fontsize=14)\n",
    "plt.ylabel('Value', fontsize=14)\n",
    "plt.legend(fontsize=12, ncols=2)\n",
    "plt.title(\"Wave data\")\n",
    "plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "with open(\"data/hw1/multimodal_data_test.pickle\", 'rb') as handle:\n",
    "    wave_data = pickle.load(handle)\n",
    "history = wave_data[\"history\"]\n",
    "future = wave_data[\"future\"]\n",
    "\n",
    "\n",
    "index = 1\n",
    "history_length = history.shape[-1]\n",
    "future_length = future.shape[-1]\n",
    "ts_history = np.arange(-history_length,0)\n",
    "ts_future = np.arange(future_length)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ts_history, history[index], marker='o', linestyle='--', label=\"History\")\n",
    "for i in range(3):\n",
    "    plt.plot([-1,0], [history[i*100 + index][-1], future[i*100 + index][0]], marker='o', linestyle='--', color=\"C0\")\n",
    "    plt.plot(ts_future, future[i*100 + index], markersize=7, marker='o', linestyle='--', label=\"Future\")\n",
    "\n",
    "plt.xlabel('Steps', fontsize=14)\n",
    "plt.ylabel('Value', fontsize=14)\n",
    "plt.legend(fontsize=12, ncols=2)\n",
    "plt.title(\"Multimodal data\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with sinusoidal trajectories\n",
    "\n",
    "In this problem, you will learn a regular MLP to regress on sinusoidal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to poke around the data\n",
    "train_data = hw1_helper.TrajectoryData(\"data/hw1/wave_data_train\")\n",
    "test_data = hw1_helper.TrajectoryData(\"data/hw1/wave_data_test\")\n",
    "\n",
    "history_length = 10\n",
    "future_length = 5\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "print(\"Train set has %i examples\"%len(train_data))\n",
    "print(\"Test set has %i examples\"%len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple MLP predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple MLP model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, history_length, future_length, hidden_size=32):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # TODO: construct MLP network\n",
    "        self.l1 = torch.nn.Linear(history_length,hidden_size) # specify layer 1\n",
    "        self.relu = torch.nn.ReLU() # defines a generic ReLU activation function, very simple\n",
    "        self.l2 = torch.nn.Linear(hidden_size, future_length) # specify layer 2\n",
    "        # self.l3 = torch.nn.Linear(hidden_size,future_length) # specify layer 3, the end of the NN\n",
    "        #############################\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Taken from a previous class' homework in which a NN classifier was trained on the iris dataset.\n",
    "\n",
    "        out = self.l1(x)  #this specifies the first layer to act on x \n",
    "        out = self.relu(out)  #specifies a relu activation on the first layer \n",
    "        out = self.l2(out)  # specifies a second layer transformation on first layer activations \n",
    "        #out = self.relu(out)  # specifies a RELU activation on second layer \n",
    "        #out = self.l3(out)  # specifies a third layer transformation\n",
    "\n",
    "        return out\n",
    "\n",
    "        # return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 8\n",
    "history_length = 10\n",
    "future_length = 5\n",
    "\n",
    "model = MLP(history_length, future_length, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = hw1_helper.train(model, optimizer, train_dataloader, criterion, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_data)-1, step=1, description='Index:')\n",
    "xlims = [-11, 5]\n",
    "ylims = [-2,2]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # HINT: Use Pytorch built-in functions for LSTM and Linear layers.\n",
    "        # HINT: batch dimension is dim=0\n",
    "        \n",
    "        # TODO: Define encoder LSTM.\n",
    "        self.encoder = None\n",
    "        ############################\n",
    "        \n",
    "        # TODO: Define decoder LSTM\n",
    "        self.decoder = None\n",
    "        ############################\n",
    "\n",
    "        \n",
    "        #TODO: Define linear project from hidden_dim to output_dim\n",
    "        self.projection = None\n",
    "        ############################\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, t_max, y=None, prob=1.):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM model.\n",
    "        x: The input sequence [batch_size, seq_len, input_dim]\n",
    "        t_max: maximum time steps to unroll\n",
    "        y: The target sequence for teacher forcing (optional, used if teacher forcing is applied) [batch_size, t_max, output_dim]\n",
    "        prob: Probability to apply teacher forcing (0 to 1). 1 means 100% teacher forcing, \n",
    "        \"\"\"\n",
    "        \n",
    "        # making sure x and y is the appropriate size.\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "        if y is not None and len(y.shape) == 2:\n",
    "            y = y.unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        ys = [] # collect outputs\n",
    "        # TODO: Run input through encoder to get initial hidden state for decoder\n",
    "\n",
    "        ############################\n",
    "\n",
    "        \n",
    "        # TODO: initial state for decoder is last input state\n",
    "\n",
    "        ############################\n",
    "\n",
    "\n",
    "        # TODO: unroll decoder \n",
    "        # TODO: if eval or no teacher forcing, use prediction from previous step\n",
    "        # TODO: if train and using teacher forcing, use prob to determine whether to use ground truth or previous prediction\n",
    "        \n",
    "        ############################\n",
    "\n",
    "        \n",
    "        return ys # [batch_size, ts_max, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 4\n",
    "future_length = 5\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "model = LSTM(input_size, output_size, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "prob = 0.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "def prob_schedule(i):\n",
    "    return 1 - jax.nn.sigmoid(20 * (i - 0.5)).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a slightly different training loop to account for teacher forcing\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    prob = prob_schedule((epoch + 1)/num_epochs)\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()        # Zero the gradients\n",
    "        output = model(data, future_length, target, prob)         # Forward pass\n",
    "        loss = criterion(output, target)  # Compute loss\n",
    "        loss.backward()              # Backpropagation\n",
    "        optimizer.step()             # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history, future_length)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# try with different prediction horizons\n",
    "prediction_horizon = 20\n",
    "prediction = model(history, prediction_horizon)\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "test_dataloader = list(DataLoader(test_data, batch_size=1, shuffle=False))\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_dataloader)-1, step=1, description='Index:')\n",
    "xlims = [-11, prediction_horizon + 2]\n",
    "ylims = [-5,5]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on multimodal data\n",
    "\n",
    "Now we repeat the same steps but with data where the future has multimodal outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multimodal data\n",
    "train_data = hw1_helper.TrajectoryData(\"data/hw1/multimodal_data_train\")\n",
    "test_data = hw1_helper.TrajectoryData(\"data/hw1/multimodal_data_test\")\n",
    "\n",
    "history_length = 11\n",
    "future_length = 10\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "print(\"Train set has %i examples\"%len(train_data))\n",
    "print(\"Test set has %i examples\"%len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple MLP predictor (multimodal data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_size = 32\n",
    "# You should be able to use your MLP class\n",
    "model = MLP(history_length, future_length, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = hw1_helper.train(model, optimizer, train_dataloader, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "test_dataloader = list(DataLoader(test_data, batch_size=1, shuffle=False))\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_dataloader)-1, step=1, description='Index:')\n",
    "xlims = [-11, 10]\n",
    "ylims = [-12, 12]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM predictor (multimodal data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 4\n",
    "# You should be able to use your LSTM class\n",
    "model = LSTM(input_size, output_size, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a slightly different training loop to account for teacher forcing\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()        # Zero the gradients\n",
    "        output = model(data, future_length, target, prob)         # Forward pass\n",
    "        loss = criterion(output, target)  # Compute loss\n",
    "        loss.backward()              # Backpropagation\n",
    "        optimizer.step()             # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history, future_length)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# TODO: try with different prediction horizons\n",
    "prediction_horizon = future_length\n",
    "prediction = model(history, prediction_horizon)\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "test_dataloader = list(DataLoader(test_data, batch_size=1, shuffle=False))\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_dataloader)-1, step=1, description='Index:')\n",
    "\n",
    "xlims = [-11, prediction_horizon + 2]\n",
    "ylims = [-12, 12]\n",
    "\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider CVAEs\n",
    "\n",
    "First, define the encoder and decoder. We will consider some simple MLP encoders. Generally, for trajectory data, it's typically more common to use RNNs or transformers, but since we are considering a small toy problem, we just consider MLP for now since it's simpler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# some simple MLP encoders. For trajectory data, it's typically more common to use RNNs or transformers\n",
    "class MLPEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=32):\n",
    "        super(MLPEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # TODO: Construct an MLP encoder\n",
    "        self.model = None   \n",
    "        ############################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "class MLPDecoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=32):\n",
    "        super(MLPDecoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # TODO: Construct an MLP encoder\n",
    "        self.model = None\n",
    "        ############################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Some helper functions\n",
    "def repeat_n(ten, n):\n",
    "    return torch.stack([ten] * n, dim=0)\n",
    "\n",
    "def beta_schedule(i):\n",
    "    return jax.nn.sigmoid(20 * (i - 0.5)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = hw1_helper.TrajectoryData(\"data/hw1/multimodal_data_train\")\n",
    "test_data = hw1_helper.TrajectoryData(\"data/hw1/multimodal_data_test\")\n",
    "\n",
    "history_length = 11\n",
    "future_length = 10\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "print(\"Train set has %i examples\"%len(train_data))\n",
    "print(\"Test set has %i examples\"%len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousCVAE(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, importance, decoder, prior):\n",
    "        '''\n",
    "        latent_dim: dimension of the continuous latent space\n",
    "        importance: network to encode the importance weight\n",
    "        decoder: network to decode the output\n",
    "        prior: network to encode the prior        \n",
    "        '''\n",
    "        \n",
    "        super(ContinuousCVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.importance = importance\n",
    "        self.decoder = decoder\n",
    "        self.prior = prior\n",
    "        \n",
    "        # TODO: Linear layers to project encoder/decoder to mean and logvar\n",
    "        self.mean_projection_encoder = None\n",
    "        self.logvar_projection_encoder = None\n",
    "        self.mean_projection_decoder = None\n",
    "        self.logvar_projection_decoder = None\n",
    "        ############################\n",
    "\n",
    "        \n",
    "    def encode_importance(self, x, y):\n",
    "        '''Computes mean and log(covariance) of q(z|x,y), assumes normal distribution'''\n",
    "        \n",
    "        # TODO: compute z_mu and z_logvar of q(z|x,y)\n",
    "        z_mu = None\n",
    "        z_logvar = None\n",
    "        ############################\n",
    "        \n",
    "        return z_mu, z_logvar\n",
    "    \n",
    "    \n",
    "    def encode_prior(self, x):\n",
    "        '''Computes mean and log(covariance) of p(z|x), assumes normal distribution'''\n",
    "        \n",
    "        # TODO: compute z_mu and z_logvar of p(z|x)\n",
    "        z_mu = None\n",
    "        z_logvar = None\n",
    "        ############################\n",
    "        \n",
    "        return z_mu, z_logvar\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, logvar, n=1):\n",
    "        '''samples from a normal distributions parameterized by mu and logvar. Uses PyTorch built-in reparameratization trick'''\n",
    "        \n",
    "        prob = torch.distributions.MultivariateNormal(loc=mu, covariance_matrix=torch.diag_embed(torch.exp(logvar)))\n",
    "        \n",
    "        return prob.rsample((n,))\n",
    "    \n",
    "    \n",
    "    def decode(self, x, z):\n",
    "        '''Computes mean and log(covariance) of p(y|x,z), assumes normal distribution'''\n",
    "        \n",
    "        # TODO: compute y_mu and y_logvar of p(y|x,z)\n",
    "        y_mu = None\n",
    "        y_logvar = None\n",
    "        ############################\n",
    "        \n",
    "        return y_mu, y_logvar\n",
    "\n",
    "    \n",
    "    def forward(self, x, y, n=1):\n",
    "        '''forward pass of the cvae model'''\n",
    "        \n",
    "        #  get p(z|x,(y))\n",
    "        if self.training:\n",
    "            z_mu, z_logvar = self.encode_importance(x, y)\n",
    "        else:\n",
    "            z_mu, z_logvar = self.encode_prior(x)\n",
    "        # sample from p(z|x,(y)) n times\n",
    "        z = self.reparameterize(z_mu, z_logvar, n)\n",
    "        # get p(y|x,z)\n",
    "        y_mu, y_logvar  = self.decode(repeat_n(x, n), z)     \n",
    "           \n",
    "        return z_mu, z_logvar, y_mu, y_logvar\n",
    "    \n",
    "\n",
    "    \n",
    "    def sample(self, x, num_samples=8, num_latent_samples=8):\n",
    "        '''samples from p(y|x,z) where z~p(z|x). Need to specify the number z and y samples to draw'''\n",
    "        \n",
    "        _, _, y_mu, y_logvar = self.forward(x, None, num_latent_samples)\n",
    "\n",
    "        return self.reparameterize(y_mu, y_logvar, num_samples)\n",
    "\n",
    "    \n",
    "    \n",
    "    def elbo(self, x, y, z_samples=1, beta=1.):\n",
    "        '''Compute ELBO for CVAE with continuous latent space. Optional: beta term that weigh kl divergence term'''\n",
    "        \n",
    "        q_mu, q_logvar, y_mu, y_logvar = self(x, y, z_samples) # get parameters for q(z|x,y) and p(y|x,z) where z~q(z|x,y)\n",
    "        p_mu, p_logvar = self.encode_prior(x) # get parameters for p(z|x)\n",
    "        \n",
    "        # construct the distributions\n",
    "        y_prob = torch.distributions.MultivariateNormal(loc=y_mu, covariance_matrix=torch.diag_embed(torch.exp(y_logvar)))  # p(y|x, z)\n",
    "        q = torch.distributions.MultivariateNormal(loc=q_mu, covariance_matrix=torch.diag_embed(torch.exp(q_logvar)))  # q(z|x,y)\n",
    "        p = torch.distributions.MultivariateNormal(loc=p_mu, covariance_matrix=torch.diag_embed(torch.exp(p_logvar)))  # p(z|x)\n",
    "        \n",
    "        loglikelihood = -y_prob.log_prob(repeat_n(y, z_samples)).mean() # log likelihood of data \n",
    "        kl_div = torch.distributions.kl.kl_divergence(q, p).mean()  # q_z * (log(q_z) - log(p_z))\n",
    "        \n",
    "        return loglikelihood + beta * kl_div\n",
    "        \n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous cvae\n",
    "# network parameters\n",
    "latent_dim = 1 # size of latent space\n",
    "history_length = 11\n",
    "future_length = 10\n",
    "hidden_dim = 8\n",
    "enc_dim = 8\n",
    "dec_dim = 8\n",
    "\n",
    "encoder = MLPEncoder(history_length + future_length, hidden_dim, enc_dim)\n",
    "prior = MLPEncoder(history_length, hidden_dim, enc_dim)\n",
    "decoder = MLPDecoder(latent_dim+history_length, future_length, dec_dim)\n",
    "\n",
    "cvae = ContinuousCVAE(latent_dim, encoder, decoder, prior)\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 1E-3\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "z_samples = 16\n",
    "cvae.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    beta = beta_schedule((epoch + 1)/ num_epochs) # we slowly increase the weighting on the KL divergence, following https://openreview.net/forum?id=Sy2fzU9gl\n",
    "    for batch_idx, (history, future) in enumerate(train_dataloader):\n",
    "        q_mu, q_logvar, y_mu, y_logvar = cvae(history, future)\n",
    "        p_mu, p_logvar = cvae.encode_prior(history)\n",
    "        optimizer.zero_grad()\n",
    "        loss = cvae.elbo(history, future, z_samples, beta)\n",
    "        loss.backward()\n",
    "        running_loss += loss.detach().cpu().numpy()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'======= Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f} =======')\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction on test data\n",
    "\n",
    "cvae.eval()\n",
    "\n",
    "num_samples = 8\n",
    "num_latent_samples = 8\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = cvae.sample(history, num_samples, num_latent_samples)\n",
    "    \n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_data)-1, step=1, description='Index:')\n",
    "xlims = [-12, 10]\n",
    "ylims = [-12, 12]\n",
    "interact(hw1_helper.plot_data_generative, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5 # choose an index you want to plot\n",
    "hw1_helper.plot_data_generative(history=history, future=future, prediction=prediction, index=index, xlims=xlims, ylims=ylims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteCVAE(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, importance, decoder, prior, num_categories):\n",
    "        '''\n",
    "        latent_dim: dimension of the continuous latent space\n",
    "        importance: network to encode the importance weight\n",
    "        decoder: network to decode the output\n",
    "        prior: network to encode the prior  \n",
    "        num_categories: number of categories per latent dimension \n",
    "        '''\n",
    "        \n",
    "        super(DiscreteCVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.importance = importance\n",
    "        self.decoder = decoder\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.logits_projection_encoder = torch.nn.Linear(importance.output_dim, latent_dim * num_categories)\n",
    "        self.mean_projection_decoder = torch.nn.Linear(decoder.output_dim, decoder.output_dim)\n",
    "        self.logvar_projection_decoder = torch.nn.Linear(decoder.output_dim, decoder.output_dim)\n",
    "        self.num_categories = num_categories\n",
    "        \n",
    "        # Gumbel-softmax reparameterization\n",
    "        self.gumbel_temperature = 0.1\n",
    "        \n",
    "    def encode_importance(self, x, y):\n",
    "        '''Computes logits of q(z|x,y), assumes one-hot categorical'''\n",
    "        xy = torch.cat([x, y], dim=-1)\n",
    "        h = self.importance(xy)\n",
    "        z_logits = self.logits_projection_encoder(h).reshape(-1, self.latent_dim, self.num_categories)      \n",
    "        return z_logits\n",
    "    \n",
    "    \n",
    "    def encode_prior(self, x):\n",
    "        '''Computes logits of p(z|x), assumes one-hot categorical'''\n",
    "        h = self.prior(x)\n",
    "        z_logits = self.logits_projection_encoder(h).reshape(-1, self.latent_dim, self.num_categories)\n",
    "        \n",
    "        return z_logits\n",
    "\n",
    "\n",
    "    def reparameterize(self, logits):\n",
    "        '''Sample latent variables using one-hot categorical distributions. Uses gumbel-softmax trick. Pytorch has a bulit-in function for this.'''\n",
    "        \n",
    "        return F.gumbel_softmax(logits, tau=self.gumbel_temperature, hard=True) \n",
    "        \n",
    "        \n",
    "    def decode(self, x, z):\n",
    "        '''Computes mean and log(covariance) of p(y|x,z), assumes normal distribution'''\n",
    "        xz = torch.cat([x, z], dim=-1)\n",
    "        g = self.decoder(xz)\n",
    "        y_mu = self.mean_projection_decoder(g)\n",
    "        y_logvar = torch.clip(self.logvar_projection_decoder(g), min=-10, max=1)\n",
    "        \n",
    "        return y_mu, y_logvar\n",
    "\n",
    "\n",
    "    def forward(self, x, y, n=1):\n",
    "        '''forward pass of the cvae model'''\n",
    "        #  get p(z|x,(y)) and samples from it n times\n",
    "        if self.training:\n",
    "            logits = self.encode_importance(x, y) # [bs, latent_dim, num_categories]\n",
    "            z = self.reparameterize(repeat_n(logits, n)) # [n, bs, latent_dim, num_categories]\n",
    "        else:\n",
    "            logits = self.encode_prior(x) # [bs, latent_dim, num_categories]\n",
    "            z = torch.distributions.OneHotCategorical(logits=logits).sample((n,)) # [n, bs, latent_dim, num_categories]\n",
    "        z_flatten = z.view(n, -1, self.latent_dim * self.num_categories)  # reshapes to [n, bs, latent_dim * num_categories]\n",
    "        # get p(y|x,z)\n",
    "        y_mu, y_logvar  = self.decode(repeat_n(x, n), z_flatten) \n",
    "\n",
    "        return logits, y_mu, y_logvar\n",
    "    \n",
    "    \n",
    "    def sample(self, x, num_samples=8, num_latent_samples=8):\n",
    "        '''samples from p(y|x,z) where z~p(z|x). Need to specify the number z and y samples to draw'''\n",
    "        _, y_mu, y_logvar = self.forward(x, None, num_latent_samples)\n",
    "\n",
    "        return  torch.distributions.MultivariateNormal(loc=y_mu, covariance_matrix=torch.diag_embed(torch.exp(y_logvar))).rsample((num_samples,))\n",
    "        \n",
    "        \n",
    "    def elbo(self, x, y, z_samples=1, beta=1.):\n",
    "        '''Compute ELBO for CVAE with discrete latent space. Optional: beta term that weigh kl divergence term'''\n",
    "\n",
    "        logits, y_mu, y_logvar = self.forward(x, y, z_samples)\n",
    "        prior_logits = cvae.encode_prior(x)\n",
    "        \n",
    "        y_prob = torch.distributions.MultivariateNormal(loc=y_mu, covariance_matrix=torch.diag_embed(torch.exp(y_logvar)))  # p(y|x, z)\n",
    "        \n",
    "        q_z = F.softmax(logits, dim=-1)  # q(z|x,y)\n",
    "        log_p_z = F.log_softmax(prior_logits, dim=-1)  # log(p(z|x))\n",
    "        \n",
    "        loglikelihood = -y_prob.log_prob(repeat_n(y, z_samples)).mean()\n",
    "        kl_div = torch.nn.KLDivLoss(reduction=\"batchmean\")(log_p_z, q_z)\n",
    "        \n",
    "        return loglikelihood + beta * kl_div\n",
    "      \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete CVAE\n",
    "\n",
    "history_length = 11\n",
    "future_length = 10\n",
    "hidden_dim = 8\n",
    "enc_dim = 8\n",
    "dec_dim = 8\n",
    "\n",
    "latent_dim = 2\n",
    "num_categories = 3\n",
    "\n",
    "encoder = MLPEncoder(history_length + future_length, hidden_dim, enc_dim)\n",
    "prior = MLPEncoder(history_length, hidden_dim, enc_dim)\n",
    "decoder = MLPDecoder(latent_dim * num_categories + history_length, future_length, dec_dim)\n",
    "\n",
    "cvae = DiscreteCVAE(latent_dim, encoder, decoder, prior, num_categories)\n",
    "\n",
    "learning_rate = 1E-3\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=learning_rate, weight_decay=1E-2)\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "cvae.train()\n",
    "num_latent_samples = 8\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for batch_idx, (history, future) in enumerate(train_dataloader):\n",
    "        beta = beta_schedule((epoch+1) / num_epochs) # we slowly increase the weighting on the KL divergence, following https://openreview.net/forum?id=Sy2fzU9gl\n",
    "        optimizer.zero_grad()\n",
    "        loss = cvae.elbo(history, future, num_latent_samples, beta)\n",
    "        loss.backward()\n",
    "        running_loss += loss.detach().cpu().numpy()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'======= Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f} =======')\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction on test data\n",
    "\n",
    "cvae.eval()\n",
    "num_latent_samples = 32\n",
    "num_samples = 1\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = cvae.sample(history, num_samples, num_latent_samples)\n",
    "    \n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_data)-1, step=1, description='Index:')\n",
    "xlims = [-12, 10]\n",
    "ylims = [-12, 12]\n",
    "interact(hw1_helper.plot_data_generative, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5 # choose an index you want to plot\n",
    "hw1_helper.plot_data_generative(history=history, future=future, prediction=prediction, index=index, xlims=xlims, ylims=ylims)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
