{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import aa598.hw1_helper as hw1_helper\n",
    "\n",
    "from matplotlib import rc\n",
    "# rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "rc('text', usetex=False) # set to False if latex is not set up on your computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"data/hw1/wave_data_train.pickle\", 'rb') as handle:\n",
    "    wave_data = pickle.load(handle)\n",
    "history = wave_data[\"history\"]\n",
    "future = wave_data[\"future\"]\n",
    "\n",
    "index = 1\n",
    "history_length = history.shape[-1]\n",
    "future_length = future.shape[-1]\n",
    "ts_history = np.arange(-history_length,0)\n",
    "ts_future = np.arange(future_length)\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(ts_history, history[index], marker='o', linestyle='--', label=\"History\")\n",
    "plt.plot([-1,0], [history[index][-1], future[index][0]], marker='o', linestyle='--', color=\"C0\")\n",
    "plt.plot(ts_future, future[index], markersize=7, marker='o', linestyle='--', label=\"Future\")\n",
    "\n",
    "plt.xlabel('Steps', fontsize=14)\n",
    "plt.ylabel('Value', fontsize=14)\n",
    "plt.legend(fontsize=12, ncols=2)\n",
    "plt.title(\"Wave data\")\n",
    "plt.grid(True)\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "with open(\"data/hw1/multimodal_data_test.pickle\", 'rb') as handle:\n",
    "    wave_data = pickle.load(handle)\n",
    "history = wave_data[\"history\"]\n",
    "future = wave_data[\"future\"]\n",
    "\n",
    "\n",
    "index = 1\n",
    "history_length = history.shape[-1]\n",
    "future_length = future.shape[-1]\n",
    "ts_history = np.arange(-history_length,0)\n",
    "ts_future = np.arange(future_length)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ts_history, history[index], marker='o', linestyle='--', label=\"History\")\n",
    "for i in range(3):\n",
    "    plt.plot([-1,0], [history[i*100 + index][-1], future[i*100 + index][0]], marker='o', linestyle='--', color=\"C0\")\n",
    "    plt.plot(ts_future, future[i*100 + index], markersize=7, marker='o', linestyle='--', label=\"Future\")\n",
    "\n",
    "plt.xlabel('Steps', fontsize=14)\n",
    "plt.ylabel('Value', fontsize=14)\n",
    "plt.legend(fontsize=12, ncols=2)\n",
    "plt.title(\"Multimodal data\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with sinusoidal trajectories\n",
    "\n",
    "In this problem, you will learn a regular MLP to regress on sinusoidal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has 10000 examples\n",
      "Test set has 500 examples\n"
     ]
    }
   ],
   "source": [
    "# feel free to poke around the data\n",
    "train_data = hw1_helper.TrajectoryData(\"data/hw1/wave_data_train\")\n",
    "test_data = hw1_helper.TrajectoryData(\"data/hw1/wave_data_test\")\n",
    "\n",
    "history_length = 10\n",
    "future_length = 5\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "print(\"Train set has %i examples\"%len(train_data))\n",
    "print(\"Test set has %i examples\"%len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple MLP predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple MLP model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, history_length, future_length, hidden_size=32):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # TODO: construct MLP network\n",
    "        self.l1 = torch.nn.Linear(history_length,hidden_size) # specify layer 1\n",
    "        self.relu = torch.nn.ReLU() # defines a generic ReLU activation function, very simple\n",
    "        self.l2 = torch.nn.Linear(hidden_size, hidden_size) # specify layer 2\n",
    "        self.l3 = torch.nn.Linear(hidden_size,future_length) # specify layer 3, the end of the NN\n",
    "        #############################\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Taken from a previous class' homework in which a NN classifier was trained on the iris dataset.\n",
    "\n",
    "        out = self.l1(x)  #this specifies the first layer to act on x \n",
    "        out = self.relu(out)  #specifies a ReLU activation on the first layer \n",
    "        out = self.l2(out)  # specifies a second layer transformation on first layer activations \n",
    "        out = self.relu(out)  # specifies a ReLU activation on second layer \n",
    "        out = self.l3(out)  # specifies a third layer transformation\n",
    "\n",
    "        return out\n",
    "\n",
    "        # return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 8\n",
    "history_length = 10\n",
    "future_length = 5\n",
    "\n",
    "model = MLP(history_length, future_length, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [0/313], Loss: 0.0021\n",
      "Epoch [1/30], Step [20/313], Loss: 0.0029\n",
      "Epoch [1/30], Step [40/313], Loss: 0.0018\n",
      "Epoch [1/30], Step [60/313], Loss: 0.0012\n",
      "Epoch [1/30], Step [80/313], Loss: 0.0024\n",
      "Epoch [1/30], Step [100/313], Loss: 0.0009\n",
      "Epoch [1/30], Step [120/313], Loss: 0.0034\n",
      "Epoch [1/30], Step [140/313], Loss: 0.0030\n",
      "Epoch [1/30], Step [160/313], Loss: 0.0021\n",
      "Epoch [1/30], Step [180/313], Loss: 0.0019\n",
      "Epoch [1/30], Step [200/313], Loss: 0.0005\n",
      "Epoch [1/30], Step [220/313], Loss: 0.0009\n",
      "Epoch [1/30], Step [240/313], Loss: 0.0012\n",
      "Epoch [1/30], Step [260/313], Loss: 0.0022\n",
      "Epoch [1/30], Step [280/313], Loss: 0.0023\n",
      "Epoch [1/30], Step [300/313], Loss: 0.0008\n",
      "Epoch 1 completed with average loss: 0.0013\n",
      "Epoch [2/30], Step [0/313], Loss: 0.0012\n",
      "Epoch [2/30], Step [20/313], Loss: 0.0013\n",
      "Epoch [2/30], Step [40/313], Loss: 0.0024\n",
      "Epoch [2/30], Step [60/313], Loss: 0.0010\n",
      "Epoch [2/30], Step [80/313], Loss: 0.0011\n",
      "Epoch [2/30], Step [100/313], Loss: 0.0010\n",
      "Epoch [2/30], Step [120/313], Loss: 0.0004\n",
      "Epoch [2/30], Step [140/313], Loss: 0.0012\n",
      "Epoch [2/30], Step [160/313], Loss: 0.0008\n",
      "Epoch [2/30], Step [180/313], Loss: 0.0017\n",
      "Epoch [2/30], Step [200/313], Loss: 0.0008\n",
      "Epoch [2/30], Step [220/313], Loss: 0.0015\n",
      "Epoch [2/30], Step [240/313], Loss: 0.0009\n",
      "Epoch [2/30], Step [260/313], Loss: 0.0028\n",
      "Epoch [2/30], Step [280/313], Loss: 0.0011\n",
      "Epoch [2/30], Step [300/313], Loss: 0.0012\n",
      "Epoch 2 completed with average loss: 0.0013\n",
      "Epoch [3/30], Step [0/313], Loss: 0.0009\n",
      "Epoch [3/30], Step [20/313], Loss: 0.0008\n",
      "Epoch [3/30], Step [40/313], Loss: 0.0010\n",
      "Epoch [3/30], Step [60/313], Loss: 0.0017\n",
      "Epoch [3/30], Step [80/313], Loss: 0.0011\n",
      "Epoch [3/30], Step [100/313], Loss: 0.0021\n",
      "Epoch [3/30], Step [120/313], Loss: 0.0014\n",
      "Epoch [3/30], Step [140/313], Loss: 0.0007\n",
      "Epoch [3/30], Step [160/313], Loss: 0.0014\n",
      "Epoch [3/30], Step [180/313], Loss: 0.0009\n",
      "Epoch [3/30], Step [200/313], Loss: 0.0007\n",
      "Epoch [3/30], Step [220/313], Loss: 0.0014\n",
      "Epoch [3/30], Step [240/313], Loss: 0.0008\n",
      "Epoch [3/30], Step [260/313], Loss: 0.0010\n",
      "Epoch [3/30], Step [280/313], Loss: 0.0007\n",
      "Epoch [3/30], Step [300/313], Loss: 0.0006\n",
      "Epoch 3 completed with average loss: 0.0012\n",
      "Epoch [4/30], Step [0/313], Loss: 0.0011\n",
      "Epoch [4/30], Step [20/313], Loss: 0.0006\n",
      "Epoch [4/30], Step [40/313], Loss: 0.0010\n",
      "Epoch [4/30], Step [60/313], Loss: 0.0009\n",
      "Epoch [4/30], Step [80/313], Loss: 0.0013\n",
      "Epoch [4/30], Step [100/313], Loss: 0.0013\n",
      "Epoch [4/30], Step [120/313], Loss: 0.0012\n",
      "Epoch [4/30], Step [140/313], Loss: 0.0014\n",
      "Epoch [4/30], Step [160/313], Loss: 0.0009\n",
      "Epoch [4/30], Step [180/313], Loss: 0.0007\n",
      "Epoch [4/30], Step [200/313], Loss: 0.0015\n",
      "Epoch [4/30], Step [220/313], Loss: 0.0010\n",
      "Epoch [4/30], Step [240/313], Loss: 0.0017\n",
      "Epoch [4/30], Step [260/313], Loss: 0.0005\n",
      "Epoch [4/30], Step [280/313], Loss: 0.0012\n",
      "Epoch [4/30], Step [300/313], Loss: 0.0013\n",
      "Epoch 4 completed with average loss: 0.0012\n",
      "Epoch [5/30], Step [0/313], Loss: 0.0026\n",
      "Epoch [5/30], Step [20/313], Loss: 0.0006\n",
      "Epoch [5/30], Step [40/313], Loss: 0.0009\n",
      "Epoch [5/30], Step [60/313], Loss: 0.0014\n",
      "Epoch [5/30], Step [80/313], Loss: 0.0012\n",
      "Epoch [5/30], Step [100/313], Loss: 0.0007\n",
      "Epoch [5/30], Step [120/313], Loss: 0.0007\n",
      "Epoch [5/30], Step [140/313], Loss: 0.0007\n",
      "Epoch [5/30], Step [160/313], Loss: 0.0018\n",
      "Epoch [5/30], Step [180/313], Loss: 0.0004\n",
      "Epoch [5/30], Step [200/313], Loss: 0.0011\n",
      "Epoch [5/30], Step [220/313], Loss: 0.0011\n",
      "Epoch [5/30], Step [240/313], Loss: 0.0007\n",
      "Epoch [5/30], Step [260/313], Loss: 0.0018\n",
      "Epoch [5/30], Step [280/313], Loss: 0.0008\n",
      "Epoch [5/30], Step [300/313], Loss: 0.0008\n",
      "Epoch 5 completed with average loss: 0.0012\n",
      "Epoch [6/30], Step [0/313], Loss: 0.0007\n",
      "Epoch [6/30], Step [20/313], Loss: 0.0011\n",
      "Epoch [6/30], Step [40/313], Loss: 0.0016\n",
      "Epoch [6/30], Step [60/313], Loss: 0.0007\n",
      "Epoch [6/30], Step [80/313], Loss: 0.0014\n",
      "Epoch [6/30], Step [100/313], Loss: 0.0008\n",
      "Epoch [6/30], Step [120/313], Loss: 0.0008\n",
      "Epoch [6/30], Step [140/313], Loss: 0.0015\n",
      "Epoch [6/30], Step [160/313], Loss: 0.0006\n",
      "Epoch [6/30], Step [180/313], Loss: 0.0009\n",
      "Epoch [6/30], Step [200/313], Loss: 0.0009\n",
      "Epoch [6/30], Step [220/313], Loss: 0.0009\n",
      "Epoch [6/30], Step [240/313], Loss: 0.0016\n",
      "Epoch [6/30], Step [260/313], Loss: 0.0033\n",
      "Epoch [6/30], Step [280/313], Loss: 0.0008\n",
      "Epoch [6/30], Step [300/313], Loss: 0.0005\n",
      "Epoch 6 completed with average loss: 0.0011\n",
      "Epoch [7/30], Step [0/313], Loss: 0.0019\n",
      "Epoch [7/30], Step [20/313], Loss: 0.0009\n",
      "Epoch [7/30], Step [40/313], Loss: 0.0006\n",
      "Epoch [7/30], Step [60/313], Loss: 0.0013\n",
      "Epoch [7/30], Step [80/313], Loss: 0.0010\n",
      "Epoch [7/30], Step [100/313], Loss: 0.0011\n",
      "Epoch [7/30], Step [120/313], Loss: 0.0006\n",
      "Epoch [7/30], Step [140/313], Loss: 0.0011\n",
      "Epoch [7/30], Step [160/313], Loss: 0.0011\n",
      "Epoch [7/30], Step [180/313], Loss: 0.0007\n",
      "Epoch [7/30], Step [200/313], Loss: 0.0014\n",
      "Epoch [7/30], Step [220/313], Loss: 0.0006\n",
      "Epoch [7/30], Step [240/313], Loss: 0.0010\n",
      "Epoch [7/30], Step [260/313], Loss: 0.0023\n",
      "Epoch [7/30], Step [280/313], Loss: 0.0005\n",
      "Epoch [7/30], Step [300/313], Loss: 0.0008\n",
      "Epoch 7 completed with average loss: 0.0011\n",
      "Epoch [8/30], Step [0/313], Loss: 0.0007\n",
      "Epoch [8/30], Step [20/313], Loss: 0.0006\n",
      "Epoch [8/30], Step [40/313], Loss: 0.0005\n",
      "Epoch [8/30], Step [60/313], Loss: 0.0007\n",
      "Epoch [8/30], Step [80/313], Loss: 0.0009\n",
      "Epoch [8/30], Step [100/313], Loss: 0.0010\n",
      "Epoch [8/30], Step [120/313], Loss: 0.0008\n",
      "Epoch [8/30], Step [140/313], Loss: 0.0004\n",
      "Epoch [8/30], Step [160/313], Loss: 0.0013\n",
      "Epoch [8/30], Step [180/313], Loss: 0.0006\n",
      "Epoch [8/30], Step [200/313], Loss: 0.0007\n",
      "Epoch [8/30], Step [220/313], Loss: 0.0018\n",
      "Epoch [8/30], Step [240/313], Loss: 0.0010\n",
      "Epoch [8/30], Step [260/313], Loss: 0.0012\n",
      "Epoch [8/30], Step [280/313], Loss: 0.0015\n",
      "Epoch [8/30], Step [300/313], Loss: 0.0005\n",
      "Epoch 8 completed with average loss: 0.0010\n",
      "Epoch [9/30], Step [0/313], Loss: 0.0015\n",
      "Epoch [9/30], Step [20/313], Loss: 0.0007\n",
      "Epoch [9/30], Step [40/313], Loss: 0.0006\n",
      "Epoch [9/30], Step [60/313], Loss: 0.0011\n",
      "Epoch [9/30], Step [80/313], Loss: 0.0009\n",
      "Epoch [9/30], Step [100/313], Loss: 0.0007\n",
      "Epoch [9/30], Step [120/313], Loss: 0.0009\n",
      "Epoch [9/30], Step [140/313], Loss: 0.0003\n",
      "Epoch [9/30], Step [160/313], Loss: 0.0013\n",
      "Epoch [9/30], Step [180/313], Loss: 0.0009\n",
      "Epoch [9/30], Step [200/313], Loss: 0.0005\n",
      "Epoch [9/30], Step [220/313], Loss: 0.0018\n",
      "Epoch [9/30], Step [240/313], Loss: 0.0005\n",
      "Epoch [9/30], Step [260/313], Loss: 0.0005\n",
      "Epoch [9/30], Step [280/313], Loss: 0.0012\n",
      "Epoch [9/30], Step [300/313], Loss: 0.0010\n",
      "Epoch 9 completed with average loss: 0.0010\n",
      "Epoch [10/30], Step [0/313], Loss: 0.0010\n",
      "Epoch [10/30], Step [20/313], Loss: 0.0005\n",
      "Epoch [10/30], Step [40/313], Loss: 0.0019\n",
      "Epoch [10/30], Step [60/313], Loss: 0.0013\n",
      "Epoch [10/30], Step [80/313], Loss: 0.0006\n",
      "Epoch [10/30], Step [100/313], Loss: 0.0003\n",
      "Epoch [10/30], Step [120/313], Loss: 0.0015\n",
      "Epoch [10/30], Step [140/313], Loss: 0.0011\n",
      "Epoch [10/30], Step [160/313], Loss: 0.0022\n",
      "Epoch [10/30], Step [180/313], Loss: 0.0005\n",
      "Epoch [10/30], Step [200/313], Loss: 0.0007\n",
      "Epoch [10/30], Step [220/313], Loss: 0.0005\n",
      "Epoch [10/30], Step [240/313], Loss: 0.0010\n",
      "Epoch [10/30], Step [260/313], Loss: 0.0006\n",
      "Epoch [10/30], Step [280/313], Loss: 0.0006\n",
      "Epoch [10/30], Step [300/313], Loss: 0.0004\n",
      "Epoch 10 completed with average loss: 0.0010\n",
      "Epoch [11/30], Step [0/313], Loss: 0.0007\n",
      "Epoch [11/30], Step [20/313], Loss: 0.0022\n",
      "Epoch [11/30], Step [40/313], Loss: 0.0007\n",
      "Epoch [11/30], Step [60/313], Loss: 0.0010\n",
      "Epoch [11/30], Step [80/313], Loss: 0.0006\n",
      "Epoch [11/30], Step [100/313], Loss: 0.0009\n",
      "Epoch [11/30], Step [120/313], Loss: 0.0008\n",
      "Epoch [11/30], Step [140/313], Loss: 0.0008\n",
      "Epoch [11/30], Step [160/313], Loss: 0.0010\n",
      "Epoch [11/30], Step [180/313], Loss: 0.0005\n",
      "Epoch [11/30], Step [200/313], Loss: 0.0007\n",
      "Epoch [11/30], Step [220/313], Loss: 0.0008\n",
      "Epoch [11/30], Step [240/313], Loss: 0.0006\n",
      "Epoch [11/30], Step [260/313], Loss: 0.0011\n",
      "Epoch [11/30], Step [280/313], Loss: 0.0013\n",
      "Epoch [11/30], Step [300/313], Loss: 0.0004\n",
      "Epoch 11 completed with average loss: 0.0009\n",
      "Epoch [12/30], Step [0/313], Loss: 0.0014\n",
      "Epoch [12/30], Step [20/313], Loss: 0.0005\n",
      "Epoch [12/30], Step [40/313], Loss: 0.0019\n",
      "Epoch [12/30], Step [60/313], Loss: 0.0005\n",
      "Epoch [12/30], Step [80/313], Loss: 0.0010\n",
      "Epoch [12/30], Step [100/313], Loss: 0.0006\n",
      "Epoch [12/30], Step [120/313], Loss: 0.0009\n",
      "Epoch [12/30], Step [140/313], Loss: 0.0010\n",
      "Epoch [12/30], Step [160/313], Loss: 0.0008\n",
      "Epoch [12/30], Step [180/313], Loss: 0.0009\n",
      "Epoch [12/30], Step [200/313], Loss: 0.0004\n",
      "Epoch [12/30], Step [220/313], Loss: 0.0005\n",
      "Epoch [12/30], Step [240/313], Loss: 0.0007\n",
      "Epoch [12/30], Step [260/313], Loss: 0.0005\n",
      "Epoch [12/30], Step [280/313], Loss: 0.0005\n",
      "Epoch [12/30], Step [300/313], Loss: 0.0008\n",
      "Epoch 12 completed with average loss: 0.0009\n",
      "Epoch [13/30], Step [0/313], Loss: 0.0006\n",
      "Epoch [13/30], Step [20/313], Loss: 0.0008\n",
      "Epoch [13/30], Step [40/313], Loss: 0.0006\n",
      "Epoch [13/30], Step [60/313], Loss: 0.0007\n",
      "Epoch [13/30], Step [80/313], Loss: 0.0008\n",
      "Epoch [13/30], Step [100/313], Loss: 0.0004\n",
      "Epoch [13/30], Step [120/313], Loss: 0.0012\n",
      "Epoch [13/30], Step [140/313], Loss: 0.0011\n",
      "Epoch [13/30], Step [160/313], Loss: 0.0005\n",
      "Epoch [13/30], Step [180/313], Loss: 0.0015\n",
      "Epoch [13/30], Step [200/313], Loss: 0.0005\n",
      "Epoch [13/30], Step [220/313], Loss: 0.0003\n",
      "Epoch [13/30], Step [240/313], Loss: 0.0004\n",
      "Epoch [13/30], Step [260/313], Loss: 0.0006\n",
      "Epoch [13/30], Step [280/313], Loss: 0.0005\n",
      "Epoch [13/30], Step [300/313], Loss: 0.0008\n",
      "Epoch 13 completed with average loss: 0.0008\n",
      "Epoch [14/30], Step [0/313], Loss: 0.0006\n",
      "Epoch [14/30], Step [20/313], Loss: 0.0007\n",
      "Epoch [14/30], Step [40/313], Loss: 0.0028\n",
      "Epoch [14/30], Step [60/313], Loss: 0.0003\n",
      "Epoch [14/30], Step [80/313], Loss: 0.0005\n",
      "Epoch [14/30], Step [100/313], Loss: 0.0007\n",
      "Epoch [14/30], Step [120/313], Loss: 0.0009\n",
      "Epoch [14/30], Step [140/313], Loss: 0.0006\n",
      "Epoch [14/30], Step [160/313], Loss: 0.0005\n",
      "Epoch [14/30], Step [180/313], Loss: 0.0007\n",
      "Epoch [14/30], Step [200/313], Loss: 0.0008\n",
      "Epoch [14/30], Step [220/313], Loss: 0.0005\n",
      "Epoch [14/30], Step [240/313], Loss: 0.0010\n",
      "Epoch [14/30], Step [260/313], Loss: 0.0005\n",
      "Epoch [14/30], Step [280/313], Loss: 0.0004\n",
      "Epoch [14/30], Step [300/313], Loss: 0.0008\n",
      "Epoch 14 completed with average loss: 0.0008\n",
      "Epoch [15/30], Step [0/313], Loss: 0.0006\n",
      "Epoch [15/30], Step [20/313], Loss: 0.0006\n",
      "Epoch [15/30], Step [40/313], Loss: 0.0004\n",
      "Epoch [15/30], Step [60/313], Loss: 0.0007\n",
      "Epoch [15/30], Step [80/313], Loss: 0.0008\n",
      "Epoch [15/30], Step [100/313], Loss: 0.0004\n",
      "Epoch [15/30], Step [120/313], Loss: 0.0004\n",
      "Epoch [15/30], Step [140/313], Loss: 0.0005\n",
      "Epoch [15/30], Step [160/313], Loss: 0.0005\n",
      "Epoch [15/30], Step [180/313], Loss: 0.0004\n",
      "Epoch [15/30], Step [200/313], Loss: 0.0010\n",
      "Epoch [15/30], Step [220/313], Loss: 0.0005\n",
      "Epoch [15/30], Step [240/313], Loss: 0.0006\n",
      "Epoch [15/30], Step [260/313], Loss: 0.0012\n",
      "Epoch [15/30], Step [280/313], Loss: 0.0012\n",
      "Epoch [15/30], Step [300/313], Loss: 0.0004\n",
      "Epoch 15 completed with average loss: 0.0008\n",
      "Epoch [16/30], Step [0/313], Loss: 0.0007\n",
      "Epoch [16/30], Step [20/313], Loss: 0.0003\n",
      "Epoch [16/30], Step [40/313], Loss: 0.0009\n",
      "Epoch [16/30], Step [60/313], Loss: 0.0007\n",
      "Epoch [16/30], Step [80/313], Loss: 0.0005\n",
      "Epoch [16/30], Step [100/313], Loss: 0.0008\n",
      "Epoch [16/30], Step [120/313], Loss: 0.0006\n",
      "Epoch [16/30], Step [140/313], Loss: 0.0006\n",
      "Epoch [16/30], Step [160/313], Loss: 0.0007\n",
      "Epoch [16/30], Step [180/313], Loss: 0.0012\n",
      "Epoch [16/30], Step [200/313], Loss: 0.0005\n",
      "Epoch [16/30], Step [220/313], Loss: 0.0011\n",
      "Epoch [16/30], Step [240/313], Loss: 0.0014\n",
      "Epoch [16/30], Step [260/313], Loss: 0.0013\n",
      "Epoch [16/30], Step [280/313], Loss: 0.0005\n",
      "Epoch [16/30], Step [300/313], Loss: 0.0006\n",
      "Epoch 16 completed with average loss: 0.0008\n",
      "Epoch [17/30], Step [0/313], Loss: 0.0011\n",
      "Epoch [17/30], Step [20/313], Loss: 0.0009\n",
      "Epoch [17/30], Step [40/313], Loss: 0.0006\n",
      "Epoch [17/30], Step [60/313], Loss: 0.0007\n",
      "Epoch [17/30], Step [80/313], Loss: 0.0007\n",
      "Epoch [17/30], Step [100/313], Loss: 0.0005\n",
      "Epoch [17/30], Step [120/313], Loss: 0.0008\n",
      "Epoch [17/30], Step [140/313], Loss: 0.0003\n",
      "Epoch [17/30], Step [160/313], Loss: 0.0012\n",
      "Epoch [17/30], Step [180/313], Loss: 0.0007\n",
      "Epoch [17/30], Step [200/313], Loss: 0.0004\n",
      "Epoch [17/30], Step [220/313], Loss: 0.0006\n",
      "Epoch [17/30], Step [240/313], Loss: 0.0017\n",
      "Epoch [17/30], Step [260/313], Loss: 0.0012\n",
      "Epoch [17/30], Step [280/313], Loss: 0.0009\n",
      "Epoch [17/30], Step [300/313], Loss: 0.0007\n",
      "Epoch 17 completed with average loss: 0.0007\n",
      "Epoch [18/30], Step [0/313], Loss: 0.0007\n",
      "Epoch [18/30], Step [20/313], Loss: 0.0005\n",
      "Epoch [18/30], Step [40/313], Loss: 0.0006\n",
      "Epoch [18/30], Step [60/313], Loss: 0.0007\n",
      "Epoch [18/30], Step [80/313], Loss: 0.0011\n",
      "Epoch [18/30], Step [100/313], Loss: 0.0006\n",
      "Epoch [18/30], Step [120/313], Loss: 0.0032\n",
      "Epoch [18/30], Step [140/313], Loss: 0.0006\n",
      "Epoch [18/30], Step [160/313], Loss: 0.0009\n",
      "Epoch [18/30], Step [180/313], Loss: 0.0004\n",
      "Epoch [18/30], Step [200/313], Loss: 0.0008\n",
      "Epoch [18/30], Step [220/313], Loss: 0.0007\n",
      "Epoch [18/30], Step [240/313], Loss: 0.0005\n",
      "Epoch [18/30], Step [260/313], Loss: 0.0009\n",
      "Epoch [18/30], Step [280/313], Loss: 0.0004\n",
      "Epoch [18/30], Step [300/313], Loss: 0.0007\n",
      "Epoch 18 completed with average loss: 0.0007\n",
      "Epoch [19/30], Step [0/313], Loss: 0.0006\n",
      "Epoch [19/30], Step [20/313], Loss: 0.0007\n",
      "Epoch [19/30], Step [40/313], Loss: 0.0006\n",
      "Epoch [19/30], Step [60/313], Loss: 0.0010\n",
      "Epoch [19/30], Step [80/313], Loss: 0.0004\n",
      "Epoch [19/30], Step [100/313], Loss: 0.0006\n",
      "Epoch [19/30], Step [120/313], Loss: 0.0005\n",
      "Epoch [19/30], Step [140/313], Loss: 0.0007\n",
      "Epoch [19/30], Step [160/313], Loss: 0.0004\n",
      "Epoch [19/30], Step [180/313], Loss: 0.0004\n",
      "Epoch [19/30], Step [200/313], Loss: 0.0008\n",
      "Epoch [19/30], Step [220/313], Loss: 0.0005\n",
      "Epoch [19/30], Step [240/313], Loss: 0.0007\n",
      "Epoch [19/30], Step [260/313], Loss: 0.0016\n",
      "Epoch [19/30], Step [280/313], Loss: 0.0004\n",
      "Epoch [19/30], Step [300/313], Loss: 0.0014\n",
      "Epoch 19 completed with average loss: 0.0007\n",
      "Epoch [20/30], Step [0/313], Loss: 0.0004\n",
      "Epoch [20/30], Step [20/313], Loss: 0.0007\n",
      "Epoch [20/30], Step [40/313], Loss: 0.0005\n",
      "Epoch [20/30], Step [60/313], Loss: 0.0008\n",
      "Epoch [20/30], Step [80/313], Loss: 0.0008\n",
      "Epoch [20/30], Step [100/313], Loss: 0.0006\n",
      "Epoch [20/30], Step [120/313], Loss: 0.0004\n",
      "Epoch [20/30], Step [140/313], Loss: 0.0003\n",
      "Epoch [20/30], Step [160/313], Loss: 0.0003\n",
      "Epoch [20/30], Step [180/313], Loss: 0.0007\n",
      "Epoch [20/30], Step [200/313], Loss: 0.0005\n",
      "Epoch [20/30], Step [220/313], Loss: 0.0008\n",
      "Epoch [20/30], Step [240/313], Loss: 0.0007\n",
      "Epoch [20/30], Step [260/313], Loss: 0.0002\n",
      "Epoch [20/30], Step [280/313], Loss: 0.0008\n",
      "Epoch [20/30], Step [300/313], Loss: 0.0007\n",
      "Epoch 20 completed with average loss: 0.0007\n",
      "Epoch [21/30], Step [0/313], Loss: 0.0009\n",
      "Epoch [21/30], Step [20/313], Loss: 0.0005\n",
      "Epoch [21/30], Step [40/313], Loss: 0.0005\n",
      "Epoch [21/30], Step [60/313], Loss: 0.0012\n",
      "Epoch [21/30], Step [80/313], Loss: 0.0007\n",
      "Epoch [21/30], Step [100/313], Loss: 0.0003\n",
      "Epoch [21/30], Step [120/313], Loss: 0.0006\n",
      "Epoch [21/30], Step [140/313], Loss: 0.0005\n",
      "Epoch [21/30], Step [160/313], Loss: 0.0005\n",
      "Epoch [21/30], Step [180/313], Loss: 0.0004\n",
      "Epoch [21/30], Step [200/313], Loss: 0.0006\n",
      "Epoch [21/30], Step [220/313], Loss: 0.0006\n",
      "Epoch [21/30], Step [240/313], Loss: 0.0008\n",
      "Epoch [21/30], Step [260/313], Loss: 0.0006\n",
      "Epoch [21/30], Step [280/313], Loss: 0.0009\n",
      "Epoch [21/30], Step [300/313], Loss: 0.0006\n",
      "Epoch 21 completed with average loss: 0.0007\n",
      "Epoch [22/30], Step [0/313], Loss: 0.0008\n",
      "Epoch [22/30], Step [20/313], Loss: 0.0007\n",
      "Epoch [22/30], Step [40/313], Loss: 0.0004\n",
      "Epoch [22/30], Step [60/313], Loss: 0.0004\n",
      "Epoch [22/30], Step [80/313], Loss: 0.0004\n",
      "Epoch [22/30], Step [100/313], Loss: 0.0017\n",
      "Epoch [22/30], Step [120/313], Loss: 0.0005\n",
      "Epoch [22/30], Step [140/313], Loss: 0.0008\n",
      "Epoch [22/30], Step [160/313], Loss: 0.0005\n",
      "Epoch [22/30], Step [180/313], Loss: 0.0027\n",
      "Epoch [22/30], Step [200/313], Loss: 0.0005\n",
      "Epoch [22/30], Step [220/313], Loss: 0.0009\n",
      "Epoch [22/30], Step [240/313], Loss: 0.0005\n",
      "Epoch [22/30], Step [260/313], Loss: 0.0011\n",
      "Epoch [22/30], Step [280/313], Loss: 0.0014\n",
      "Epoch [22/30], Step [300/313], Loss: 0.0010\n",
      "Epoch 22 completed with average loss: 0.0007\n",
      "Epoch [23/30], Step [0/313], Loss: 0.0004\n",
      "Epoch [23/30], Step [20/313], Loss: 0.0006\n",
      "Epoch [23/30], Step [40/313], Loss: 0.0003\n",
      "Epoch [23/30], Step [60/313], Loss: 0.0003\n",
      "Epoch [23/30], Step [80/313], Loss: 0.0006\n",
      "Epoch [23/30], Step [100/313], Loss: 0.0004\n",
      "Epoch [23/30], Step [120/313], Loss: 0.0005\n",
      "Epoch [23/30], Step [140/313], Loss: 0.0003\n",
      "Epoch [23/30], Step [160/313], Loss: 0.0006\n",
      "Epoch [23/30], Step [180/313], Loss: 0.0006\n",
      "Epoch [23/30], Step [200/313], Loss: 0.0012\n",
      "Epoch [23/30], Step [220/313], Loss: 0.0003\n",
      "Epoch [23/30], Step [240/313], Loss: 0.0006\n",
      "Epoch [23/30], Step [260/313], Loss: 0.0003\n",
      "Epoch [23/30], Step [280/313], Loss: 0.0004\n",
      "Epoch [23/30], Step [300/313], Loss: 0.0004\n",
      "Epoch 23 completed with average loss: 0.0006\n",
      "Epoch [24/30], Step [0/313], Loss: 0.0003\n",
      "Epoch [24/30], Step [20/313], Loss: 0.0006\n",
      "Epoch [24/30], Step [40/313], Loss: 0.0007\n",
      "Epoch [24/30], Step [60/313], Loss: 0.0006\n",
      "Epoch [24/30], Step [80/313], Loss: 0.0003\n",
      "Epoch [24/30], Step [100/313], Loss: 0.0005\n",
      "Epoch [24/30], Step [120/313], Loss: 0.0004\n",
      "Epoch [24/30], Step [140/313], Loss: 0.0008\n",
      "Epoch [24/30], Step [160/313], Loss: 0.0008\n",
      "Epoch [24/30], Step [180/313], Loss: 0.0007\n",
      "Epoch [24/30], Step [200/313], Loss: 0.0007\n",
      "Epoch [24/30], Step [220/313], Loss: 0.0009\n",
      "Epoch [24/30], Step [240/313], Loss: 0.0004\n",
      "Epoch [24/30], Step [260/313], Loss: 0.0007\n",
      "Epoch [24/30], Step [280/313], Loss: 0.0010\n",
      "Epoch [24/30], Step [300/313], Loss: 0.0005\n",
      "Epoch 24 completed with average loss: 0.0006\n",
      "Epoch [25/30], Step [0/313], Loss: 0.0007\n",
      "Epoch [25/30], Step [20/313], Loss: 0.0006\n",
      "Epoch [25/30], Step [40/313], Loss: 0.0008\n",
      "Epoch [25/30], Step [60/313], Loss: 0.0004\n",
      "Epoch [25/30], Step [80/313], Loss: 0.0005\n",
      "Epoch [25/30], Step [100/313], Loss: 0.0004\n",
      "Epoch [25/30], Step [120/313], Loss: 0.0006\n",
      "Epoch [25/30], Step [140/313], Loss: 0.0003\n",
      "Epoch [25/30], Step [160/313], Loss: 0.0007\n",
      "Epoch [25/30], Step [180/313], Loss: 0.0007\n",
      "Epoch [25/30], Step [200/313], Loss: 0.0007\n",
      "Epoch [25/30], Step [220/313], Loss: 0.0023\n",
      "Epoch [25/30], Step [240/313], Loss: 0.0004\n",
      "Epoch [25/30], Step [260/313], Loss: 0.0003\n",
      "Epoch [25/30], Step [280/313], Loss: 0.0005\n",
      "Epoch [25/30], Step [300/313], Loss: 0.0004\n",
      "Epoch 25 completed with average loss: 0.0006\n",
      "Epoch [26/30], Step [0/313], Loss: 0.0007\n",
      "Epoch [26/30], Step [20/313], Loss: 0.0008\n",
      "Epoch [26/30], Step [40/313], Loss: 0.0005\n",
      "Epoch [26/30], Step [60/313], Loss: 0.0003\n",
      "Epoch [26/30], Step [80/313], Loss: 0.0005\n",
      "Epoch [26/30], Step [100/313], Loss: 0.0005\n",
      "Epoch [26/30], Step [120/313], Loss: 0.0014\n",
      "Epoch [26/30], Step [140/313], Loss: 0.0006\n",
      "Epoch [26/30], Step [160/313], Loss: 0.0004\n",
      "Epoch [26/30], Step [180/313], Loss: 0.0004\n",
      "Epoch [26/30], Step [200/313], Loss: 0.0004\n",
      "Epoch [26/30], Step [220/313], Loss: 0.0005\n",
      "Epoch [26/30], Step [240/313], Loss: 0.0006\n",
      "Epoch [26/30], Step [260/313], Loss: 0.0011\n",
      "Epoch [26/30], Step [280/313], Loss: 0.0005\n",
      "Epoch [26/30], Step [300/313], Loss: 0.0006\n",
      "Epoch 26 completed with average loss: 0.0006\n",
      "Epoch [27/30], Step [0/313], Loss: 0.0006\n",
      "Epoch [27/30], Step [20/313], Loss: 0.0006\n",
      "Epoch [27/30], Step [40/313], Loss: 0.0007\n",
      "Epoch [27/30], Step [60/313], Loss: 0.0007\n",
      "Epoch [27/30], Step [80/313], Loss: 0.0003\n",
      "Epoch [27/30], Step [100/313], Loss: 0.0008\n",
      "Epoch [27/30], Step [120/313], Loss: 0.0004\n",
      "Epoch [27/30], Step [140/313], Loss: 0.0004\n",
      "Epoch [27/30], Step [160/313], Loss: 0.0006\n",
      "Epoch [27/30], Step [180/313], Loss: 0.0005\n",
      "Epoch [27/30], Step [200/313], Loss: 0.0004\n",
      "Epoch [27/30], Step [220/313], Loss: 0.0008\n",
      "Epoch [27/30], Step [240/313], Loss: 0.0004\n",
      "Epoch [27/30], Step [260/313], Loss: 0.0006\n",
      "Epoch [27/30], Step [280/313], Loss: 0.0007\n",
      "Epoch [27/30], Step [300/313], Loss: 0.0008\n",
      "Epoch 27 completed with average loss: 0.0006\n",
      "Epoch [28/30], Step [0/313], Loss: 0.0008\n",
      "Epoch [28/30], Step [20/313], Loss: 0.0003\n",
      "Epoch [28/30], Step [40/313], Loss: 0.0006\n",
      "Epoch [28/30], Step [60/313], Loss: 0.0005\n",
      "Epoch [28/30], Step [80/313], Loss: 0.0002\n",
      "Epoch [28/30], Step [100/313], Loss: 0.0005\n",
      "Epoch [28/30], Step [120/313], Loss: 0.0005\n",
      "Epoch [28/30], Step [140/313], Loss: 0.0008\n",
      "Epoch [28/30], Step [160/313], Loss: 0.0006\n",
      "Epoch [28/30], Step [180/313], Loss: 0.0007\n",
      "Epoch [28/30], Step [200/313], Loss: 0.0004\n",
      "Epoch [28/30], Step [220/313], Loss: 0.0005\n",
      "Epoch [28/30], Step [240/313], Loss: 0.0006\n",
      "Epoch [28/30], Step [260/313], Loss: 0.0003\n",
      "Epoch [28/30], Step [280/313], Loss: 0.0007\n",
      "Epoch [28/30], Step [300/313], Loss: 0.0008\n",
      "Epoch 28 completed with average loss: 0.0006\n",
      "Epoch [29/30], Step [0/313], Loss: 0.0005\n",
      "Epoch [29/30], Step [20/313], Loss: 0.0003\n",
      "Epoch [29/30], Step [40/313], Loss: 0.0004\n",
      "Epoch [29/30], Step [60/313], Loss: 0.0005\n",
      "Epoch [29/30], Step [80/313], Loss: 0.0004\n",
      "Epoch [29/30], Step [100/313], Loss: 0.0006\n",
      "Epoch [29/30], Step [120/313], Loss: 0.0003\n",
      "Epoch [29/30], Step [140/313], Loss: 0.0005\n",
      "Epoch [29/30], Step [160/313], Loss: 0.0005\n",
      "Epoch [29/30], Step [180/313], Loss: 0.0006\n",
      "Epoch [29/30], Step [200/313], Loss: 0.0007\n",
      "Epoch [29/30], Step [220/313], Loss: 0.0007\n",
      "Epoch [29/30], Step [240/313], Loss: 0.0004\n",
      "Epoch [29/30], Step [260/313], Loss: 0.0009\n",
      "Epoch [29/30], Step [280/313], Loss: 0.0006\n",
      "Epoch [29/30], Step [300/313], Loss: 0.0002\n",
      "Epoch 29 completed with average loss: 0.0006\n",
      "Epoch [30/30], Step [0/313], Loss: 0.0003\n",
      "Epoch [30/30], Step [20/313], Loss: 0.0003\n",
      "Epoch [30/30], Step [40/313], Loss: 0.0005\n",
      "Epoch [30/30], Step [60/313], Loss: 0.0043\n",
      "Epoch [30/30], Step [80/313], Loss: 0.0006\n",
      "Epoch [30/30], Step [100/313], Loss: 0.0005\n",
      "Epoch [30/30], Step [120/313], Loss: 0.0005\n",
      "Epoch [30/30], Step [140/313], Loss: 0.0004\n",
      "Epoch [30/30], Step [160/313], Loss: 0.0005\n",
      "Epoch [30/30], Step [180/313], Loss: 0.0003\n",
      "Epoch [30/30], Step [200/313], Loss: 0.0005\n",
      "Epoch [30/30], Step [220/313], Loss: 0.0005\n",
      "Epoch [30/30], Step [240/313], Loss: 0.0003\n",
      "Epoch [30/30], Step [260/313], Loss: 0.0009\n",
      "Epoch [30/30], Step [280/313], Loss: 0.0003\n",
      "Epoch [30/30], Step [300/313], Loss: 0.0005\n",
      "Epoch 30 completed with average loss: 0.0006\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = hw1_helper.train(model, optimizer, train_dataloader, criterion, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5a358735ef49bab0d2c15f9c6e0008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Index:', max=499), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function aa598.hw1_helper.plot_data_regression(history, future, prediction, index, xlims=[-11, 5], ylims=[-2, 2])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_data)-1, step=1, description='Index:')\n",
    "xlims = [-11, 5]\n",
    "ylims = [-2,2]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n",
    "# Test Loss: 0.0009"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # HINT: Use Pytorch built-in functions for LSTM and Linear layers.\n",
    "        # HINT: batch dimension is dim=0\n",
    "        \n",
    "        # TODO: Define encoder LSTM.\n",
    "        self.encoder = torch.nn.LSTM(input_dim,hidden_dim,batch_first=True)\n",
    "        ############################\n",
    "        \n",
    "        # TODO: Define decoder LSTM\n",
    "        self.decoder = torch.nn.LSTM(hidden_dim,hidden_dim,batch_first=True)\n",
    "        ############################\n",
    "\n",
    "        \n",
    "        #TODO: Define linear project from hidden_dim to output_dim\n",
    "        self.projection = torch.nn.Linear(hidden_dim,output_dim)\n",
    "        ############################\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, t_max, y=None, prob=1.):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM model.\n",
    "        x: The input sequence [batch_size, seq_len, input_dim]\n",
    "        t_max: maximum time steps to unroll\n",
    "        y: The target sequence for teacher forcing (optional, used if teacher forcing is applied) [batch_size, t_max, output_dim]\n",
    "        prob: Probability to apply teacher forcing (0 to 1). 1 means 100% teacher forcing, \n",
    "        \"\"\"\n",
    "        \n",
    "        # making sure x and y is the appropriate size.\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(-1)\n",
    "        if y is not None and len(y.shape) == 2:\n",
    "            y = y.unsqueeze(-1)\n",
    "        \n",
    "        \n",
    "        ys = [] # collect outputs\n",
    "        # TODO: Run input through encoder to get initial hidden state for decoder\n",
    "\n",
    "        encoder_state = self.encoder(x)\n",
    "\n",
    "        ############################\n",
    "\n",
    "        \n",
    "        # TODO: initial state for decoder is last input state\n",
    "\n",
    "        decoder_state = encoder_state\n",
    "\n",
    "        ############################\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        decoder_input = torch.zeros(batch_size,1,self.hidden_dim).to(x.device)\n",
    "\n",
    "        # TODO: unroll decoder \n",
    "        for t in range(t_max):\n",
    "            decoder_output, decoder_state = self.decoder(decoder_input, decoder_state)\n",
    "            output = self.projection(decoder_output)\n",
    "            ys.append(output)\n",
    "\n",
    "\n",
    "        # TODO: if eval or no teacher forcing, use prediction from previous step\n",
    "            if self.training and y is not None and torch.rand(1).item() < prob:\n",
    "                decoder_input = y[:, t+t+1, :]\n",
    "            else:\n",
    "                decoder_input = output\n",
    "        # TODO: if train and using teacher forcing, use prob to determine whether to use ground truth or previous prediction\n",
    "        \n",
    "        ############################\n",
    "\n",
    "        #ys = torch.cat(ys, dim=1)\n",
    "        return ys # [batch_size, ts_max, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 4\n",
    "future_length = 5\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "model = LSTM(input_size, output_size, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "prob = 0.\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "def prob_schedule(i):\n",
    "    return 1 - jax.nn.sigmoid(20 * (i - 0.5)).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()        \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()              \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rfeng\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rfeng\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[33], line 63\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x, t_max, y, prob)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# TODO: unroll decoder \u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(t_max):\n\u001b[1;32m---> 63\u001b[0m     decoder_output, decoder_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(decoder_output)\n\u001b[0;32m     65\u001b[0m     ys\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "File \u001b[1;32mc:\\Users\\rfeng\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rfeng\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rfeng\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1103\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n\u001b[1;32m-> 1103\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mhx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m   1104\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1105\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor batched 3-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1106\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 3-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1107\u001b[0m             )\n\u001b[0;32m   1108\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "# we use a slightly different training loop to account for teacher forcing\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    prob = prob_schedule((epoch + 1)/num_epochs)\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()        # Zero the gradients\n",
    "        output = model(data, future_length, target, prob)         # Forward pass\n",
    "        loss = criterion(output, target)  # Compute loss\n",
    "        loss.backward()              # Backpropagation\n",
    "        optimizer.step()             # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history, future_length)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# try with different prediction horizons\n",
    "prediction_horizon = 20\n",
    "prediction = model(history, prediction_horizon)\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "test_dataloader = list(DataLoader(test_data, batch_size=1, shuffle=False))\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_dataloader)-1, step=1, description='Index:')\n",
    "xlims = [-11, prediction_horizon + 2]\n",
    "ylims = [-5,5]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression on multimodal data\n",
    "\n",
    "Now we repeat the same steps but with data where the future has multimodal outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multimodal data\n",
    "train_data = hw1_helper.TrajectoryData(\"data/hw1/multimodal_data_train\")\n",
    "test_data = hw1_helper.TrajectoryData(\"data/hw1/multimodal_data_test\")\n",
    "\n",
    "history_length = 11\n",
    "future_length = 10\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "print(\"Train set has %i examples\"%len(train_data))\n",
    "print(\"Test set has %i examples\"%len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple MLP predictor (multimodal data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_size = 32\n",
    "# You should be able to use your MLP class\n",
    "model = MLP(history_length, future_length, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = hw1_helper.train(model, optimizer, train_dataloader, criterion, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "test_dataloader = list(DataLoader(test_data, batch_size=1, shuffle=False))\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_dataloader)-1, step=1, description='Index:')\n",
    "xlims = [-11, 10]\n",
    "ylims = [-12, 12]\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM predictor (multimodal data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 4\n",
    "# You should be able to use your LSTM class\n",
    "model = LSTM(input_size, output_size, hidden_size)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use a slightly different training loop to account for teacher forcing\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()        # Zero the gradients\n",
    "        output = model(data, future_length, target, prob)         # Forward pass\n",
    "        loss = criterion(output, target)  # Compute loss\n",
    "        loss.backward()              # Backpropagation\n",
    "        optimizer.step()             # Update weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f}')\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on a test set. \n",
    "model.eval()\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = model(history, future_length)         # Forward pass\n",
    "    loss = criterion(prediction, future)  # Compute loss\n",
    "    \n",
    "# print out test loss\n",
    "print(f'Test Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# TODO: try with different prediction horizons\n",
    "prediction_horizon = future_length\n",
    "prediction = model(history, prediction_horizon)\n",
    "\n",
    "\n",
    "# Visualize prediction on test data\n",
    "test_dataloader = list(DataLoader(test_data, batch_size=1, shuffle=False))\n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_dataloader)-1, step=1, description='Index:')\n",
    "\n",
    "xlims = [-11, prediction_horizon + 2]\n",
    "ylims = [-12, 12]\n",
    "\n",
    "interact(hw1_helper.plot_data_regression, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider CVAEs\n",
    "\n",
    "First, define the encoder and decoder. We will consider some simple MLP encoders. Generally, for trajectory data, it's typically more common to use RNNs or transformers, but since we are considering a small toy problem, we just consider MLP for now since it's simpler.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# some simple MLP encoders. For trajectory data, it's typically more common to use RNNs or transformers\n",
    "class MLPEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=32):\n",
    "        super(MLPEncoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # TODO: Construct an MLP encoder\n",
    "        self.model = None   \n",
    "        ############################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "class MLPDecoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=32):\n",
    "        super(MLPDecoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # TODO: Construct an MLP encoder\n",
    "        self.model = None\n",
    "        ############################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Some helper functions\n",
    "def repeat_n(ten, n):\n",
    "    return torch.stack([ten] * n, dim=0)\n",
    "\n",
    "def beta_schedule(i):\n",
    "    return jax.nn.sigmoid(20 * (i - 0.5)).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = hw1_helper.TrajectoryData(\"data/hw1/multimodal_data_train\")\n",
    "test_data = hw1_helper.TrajectoryData(\"data/hw1/multimodal_data_test\")\n",
    "\n",
    "history_length = 11\n",
    "future_length = 10\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "\n",
    "print(\"Train set has %i examples\"%len(train_data))\n",
    "print(\"Test set has %i examples\"%len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousCVAE(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, importance, decoder, prior):\n",
    "        '''\n",
    "        latent_dim: dimension of the continuous latent space\n",
    "        importance: network to encode the importance weight\n",
    "        decoder: network to decode the output\n",
    "        prior: network to encode the prior        \n",
    "        '''\n",
    "        \n",
    "        super(ContinuousCVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.importance = importance\n",
    "        self.decoder = decoder\n",
    "        self.prior = prior\n",
    "        \n",
    "        # TODO: Linear layers to project encoder/decoder to mean and logvar\n",
    "        self.mean_projection_encoder = None\n",
    "        self.logvar_projection_encoder = None\n",
    "        self.mean_projection_decoder = None\n",
    "        self.logvar_projection_decoder = None\n",
    "        ############################\n",
    "\n",
    "        \n",
    "    def encode_importance(self, x, y):\n",
    "        '''Computes mean and log(covariance) of q(z|x,y), assumes normal distribution'''\n",
    "        \n",
    "        # TODO: compute z_mu and z_logvar of q(z|x,y)\n",
    "        z_mu = None\n",
    "        z_logvar = None\n",
    "        ############################\n",
    "        \n",
    "        return z_mu, z_logvar\n",
    "    \n",
    "    \n",
    "    def encode_prior(self, x):\n",
    "        '''Computes mean and log(covariance) of p(z|x), assumes normal distribution'''\n",
    "        \n",
    "        # TODO: compute z_mu and z_logvar of p(z|x)\n",
    "        z_mu = None\n",
    "        z_logvar = None\n",
    "        ############################\n",
    "        \n",
    "        return z_mu, z_logvar\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, logvar, n=1):\n",
    "        '''samples from a normal distributions parameterized by mu and logvar. Uses PyTorch built-in reparameratization trick'''\n",
    "        \n",
    "        prob = torch.distributions.MultivariateNormal(loc=mu, covariance_matrix=torch.diag_embed(torch.exp(logvar)))\n",
    "        \n",
    "        return prob.rsample((n,))\n",
    "    \n",
    "    \n",
    "    def decode(self, x, z):\n",
    "        '''Computes mean and log(covariance) of p(y|x,z), assumes normal distribution'''\n",
    "        \n",
    "        # TODO: compute y_mu and y_logvar of p(y|x,z)\n",
    "        y_mu = None\n",
    "        y_logvar = None\n",
    "        ############################\n",
    "        \n",
    "        return y_mu, y_logvar\n",
    "\n",
    "    \n",
    "    def forward(self, x, y, n=1):\n",
    "        '''forward pass of the cvae model'''\n",
    "        \n",
    "        #  get p(z|x,(y))\n",
    "        if self.training:\n",
    "            z_mu, z_logvar = self.encode_importance(x, y)\n",
    "        else:\n",
    "            z_mu, z_logvar = self.encode_prior(x)\n",
    "        # sample from p(z|x,(y)) n times\n",
    "        z = self.reparameterize(z_mu, z_logvar, n)\n",
    "        # get p(y|x,z)\n",
    "        y_mu, y_logvar  = self.decode(repeat_n(x, n), z)     \n",
    "           \n",
    "        return z_mu, z_logvar, y_mu, y_logvar\n",
    "    \n",
    "\n",
    "    \n",
    "    def sample(self, x, num_samples=8, num_latent_samples=8):\n",
    "        '''samples from p(y|x,z) where z~p(z|x). Need to specify the number z and y samples to draw'''\n",
    "        \n",
    "        _, _, y_mu, y_logvar = self.forward(x, None, num_latent_samples)\n",
    "\n",
    "        return self.reparameterize(y_mu, y_logvar, num_samples)\n",
    "\n",
    "    \n",
    "    \n",
    "    def elbo(self, x, y, z_samples=1, beta=1.):\n",
    "        '''Compute ELBO for CVAE with continuous latent space. Optional: beta term that weigh kl divergence term'''\n",
    "        \n",
    "        q_mu, q_logvar, y_mu, y_logvar = self(x, y, z_samples) # get parameters for q(z|x,y) and p(y|x,z) where z~q(z|x,y)\n",
    "        p_mu, p_logvar = self.encode_prior(x) # get parameters for p(z|x)\n",
    "        \n",
    "        # construct the distributions\n",
    "        y_prob = torch.distributions.MultivariateNormal(loc=y_mu, covariance_matrix=torch.diag_embed(torch.exp(y_logvar)))  # p(y|x, z)\n",
    "        q = torch.distributions.MultivariateNormal(loc=q_mu, covariance_matrix=torch.diag_embed(torch.exp(q_logvar)))  # q(z|x,y)\n",
    "        p = torch.distributions.MultivariateNormal(loc=p_mu, covariance_matrix=torch.diag_embed(torch.exp(p_logvar)))  # p(z|x)\n",
    "        \n",
    "        loglikelihood = -y_prob.log_prob(repeat_n(y, z_samples)).mean() # log likelihood of data \n",
    "        kl_div = torch.distributions.kl.kl_divergence(q, p).mean()  # q_z * (log(q_z) - log(p_z))\n",
    "        \n",
    "        return loglikelihood + beta * kl_div\n",
    "        \n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous cvae\n",
    "# network parameters\n",
    "latent_dim = 1 # size of latent space\n",
    "history_length = 11\n",
    "future_length = 10\n",
    "hidden_dim = 8\n",
    "enc_dim = 8\n",
    "dec_dim = 8\n",
    "\n",
    "encoder = MLPEncoder(history_length + future_length, hidden_dim, enc_dim)\n",
    "prior = MLPEncoder(history_length, hidden_dim, enc_dim)\n",
    "decoder = MLPDecoder(latent_dim+history_length, future_length, dec_dim)\n",
    "\n",
    "cvae = ContinuousCVAE(latent_dim, encoder, decoder, prior)\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 1E-3\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "z_samples = 16\n",
    "cvae.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    beta = beta_schedule((epoch + 1)/ num_epochs) # we slowly increase the weighting on the KL divergence, following https://openreview.net/forum?id=Sy2fzU9gl\n",
    "    for batch_idx, (history, future) in enumerate(train_dataloader):\n",
    "        q_mu, q_logvar, y_mu, y_logvar = cvae(history, future)\n",
    "        p_mu, p_logvar = cvae.encode_prior(history)\n",
    "        optimizer.zero_grad()\n",
    "        loss = cvae.elbo(history, future, z_samples, beta)\n",
    "        loss.backward()\n",
    "        running_loss += loss.detach().cpu().numpy()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'======= Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f} =======')\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction on test data\n",
    "\n",
    "cvae.eval()\n",
    "\n",
    "num_samples = 8\n",
    "num_latent_samples = 8\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = cvae.sample(history, num_samples, num_latent_samples)\n",
    "    \n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_data)-1, step=1, description='Index:')\n",
    "xlims = [-12, 10]\n",
    "ylims = [-12, 12]\n",
    "interact(hw1_helper.plot_data_generative, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5 # choose an index you want to plot\n",
    "hw1_helper.plot_data_generative(history=history, future=future, prediction=prediction, index=index, xlims=xlims, ylims=ylims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteCVAE(torch.nn.Module):\n",
    "    def __init__(self, latent_dim, importance, decoder, prior, num_categories):\n",
    "        '''\n",
    "        latent_dim: dimension of the continuous latent space\n",
    "        importance: network to encode the importance weight\n",
    "        decoder: network to decode the output\n",
    "        prior: network to encode the prior  \n",
    "        num_categories: number of categories per latent dimension \n",
    "        '''\n",
    "        \n",
    "        super(DiscreteCVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.importance = importance\n",
    "        self.decoder = decoder\n",
    "        self.prior = prior\n",
    "        \n",
    "        self.logits_projection_encoder = torch.nn.Linear(importance.output_dim, latent_dim * num_categories)\n",
    "        self.mean_projection_decoder = torch.nn.Linear(decoder.output_dim, decoder.output_dim)\n",
    "        self.logvar_projection_decoder = torch.nn.Linear(decoder.output_dim, decoder.output_dim)\n",
    "        self.num_categories = num_categories\n",
    "        \n",
    "        # Gumbel-softmax reparameterization\n",
    "        self.gumbel_temperature = 0.1\n",
    "        \n",
    "    def encode_importance(self, x, y):\n",
    "        '''Computes logits of q(z|x,y), assumes one-hot categorical'''\n",
    "        xy = torch.cat([x, y], dim=-1)\n",
    "        h = self.importance(xy)\n",
    "        z_logits = self.logits_projection_encoder(h).reshape(-1, self.latent_dim, self.num_categories)      \n",
    "        return z_logits\n",
    "    \n",
    "    \n",
    "    def encode_prior(self, x):\n",
    "        '''Computes logits of p(z|x), assumes one-hot categorical'''\n",
    "        h = self.prior(x)\n",
    "        z_logits = self.logits_projection_encoder(h).reshape(-1, self.latent_dim, self.num_categories)\n",
    "        \n",
    "        return z_logits\n",
    "\n",
    "\n",
    "    def reparameterize(self, logits):\n",
    "        '''Sample latent variables using one-hot categorical distributions. Uses gumbel-softmax trick. Pytorch has a bulit-in function for this.'''\n",
    "        \n",
    "        return F.gumbel_softmax(logits, tau=self.gumbel_temperature, hard=True) \n",
    "        \n",
    "        \n",
    "    def decode(self, x, z):\n",
    "        '''Computes mean and log(covariance) of p(y|x,z), assumes normal distribution'''\n",
    "        xz = torch.cat([x, z], dim=-1)\n",
    "        g = self.decoder(xz)\n",
    "        y_mu = self.mean_projection_decoder(g)\n",
    "        y_logvar = torch.clip(self.logvar_projection_decoder(g), min=-10, max=1)\n",
    "        \n",
    "        return y_mu, y_logvar\n",
    "\n",
    "\n",
    "    def forward(self, x, y, n=1):\n",
    "        '''forward pass of the cvae model'''\n",
    "        #  get p(z|x,(y)) and samples from it n times\n",
    "        if self.training:\n",
    "            logits = self.encode_importance(x, y) # [bs, latent_dim, num_categories]\n",
    "            z = self.reparameterize(repeat_n(logits, n)) # [n, bs, latent_dim, num_categories]\n",
    "        else:\n",
    "            logits = self.encode_prior(x) # [bs, latent_dim, num_categories]\n",
    "            z = torch.distributions.OneHotCategorical(logits=logits).sample((n,)) # [n, bs, latent_dim, num_categories]\n",
    "        z_flatten = z.view(n, -1, self.latent_dim * self.num_categories)  # reshapes to [n, bs, latent_dim * num_categories]\n",
    "        # get p(y|x,z)\n",
    "        y_mu, y_logvar  = self.decode(repeat_n(x, n), z_flatten) \n",
    "\n",
    "        return logits, y_mu, y_logvar\n",
    "    \n",
    "    \n",
    "    def sample(self, x, num_samples=8, num_latent_samples=8):\n",
    "        '''samples from p(y|x,z) where z~p(z|x). Need to specify the number z and y samples to draw'''\n",
    "        _, y_mu, y_logvar = self.forward(x, None, num_latent_samples)\n",
    "\n",
    "        return  torch.distributions.MultivariateNormal(loc=y_mu, covariance_matrix=torch.diag_embed(torch.exp(y_logvar))).rsample((num_samples,))\n",
    "        \n",
    "        \n",
    "    def elbo(self, x, y, z_samples=1, beta=1.):\n",
    "        '''Compute ELBO for CVAE with discrete latent space. Optional: beta term that weigh kl divergence term'''\n",
    "\n",
    "        logits, y_mu, y_logvar = self.forward(x, y, z_samples)\n",
    "        prior_logits = cvae.encode_prior(x)\n",
    "        \n",
    "        y_prob = torch.distributions.MultivariateNormal(loc=y_mu, covariance_matrix=torch.diag_embed(torch.exp(y_logvar)))  # p(y|x, z)\n",
    "        \n",
    "        q_z = F.softmax(logits, dim=-1)  # q(z|x,y)\n",
    "        log_p_z = F.log_softmax(prior_logits, dim=-1)  # log(p(z|x))\n",
    "        \n",
    "        loglikelihood = -y_prob.log_prob(repeat_n(y, z_samples)).mean()\n",
    "        kl_div = torch.nn.KLDivLoss(reduction=\"batchmean\")(log_p_z, q_z)\n",
    "        \n",
    "        return loglikelihood + beta * kl_div\n",
    "      \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete CVAE\n",
    "\n",
    "history_length = 11\n",
    "future_length = 10\n",
    "hidden_dim = 8\n",
    "enc_dim = 8\n",
    "dec_dim = 8\n",
    "\n",
    "latent_dim = 2\n",
    "num_categories = 3\n",
    "\n",
    "encoder = MLPEncoder(history_length + future_length, hidden_dim, enc_dim)\n",
    "prior = MLPEncoder(history_length, hidden_dim, enc_dim)\n",
    "decoder = MLPDecoder(latent_dim * num_categories + history_length, future_length, dec_dim)\n",
    "\n",
    "cvae = DiscreteCVAE(latent_dim, encoder, decoder, prior, num_categories)\n",
    "\n",
    "learning_rate = 1E-3\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=learning_rate, weight_decay=1E-2)\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "cvae.train()\n",
    "num_latent_samples = 8\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0\n",
    "    for batch_idx, (history, future) in enumerate(train_dataloader):\n",
    "        beta = beta_schedule((epoch+1) / num_epochs) # we slowly increase the weighting on the KL divergence, following https://openreview.net/forum?id=Sy2fzU9gl\n",
    "        optimizer.zero_grad()\n",
    "        loss = cvae.elbo(history, future, num_latent_samples, beta)\n",
    "        loss.backward()\n",
    "        running_loss += loss.detach().cpu().numpy()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'======= Epoch {epoch+1} completed with average loss: {running_loss/len(train_dataloader):.4f} =======')\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction on test data\n",
    "\n",
    "cvae.eval()\n",
    "num_latent_samples = 32\n",
    "num_samples = 1\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n",
    "for (history, future) in test_dataloader:\n",
    "    prediction = cvae.sample(history, num_samples, num_latent_samples)\n",
    "    \n",
    "index_slider = widgets.IntSlider(value=0, min=0, max=len(test_data)-1, step=1, description='Index:')\n",
    "xlims = [-12, 10]\n",
    "ylims = [-12, 12]\n",
    "interact(hw1_helper.plot_data_generative, history=widgets.fixed(history), future=widgets.fixed(future), prediction=widgets.fixed(prediction), index=index_slider, xlims=widgets.fixed(xlims), ylims=widgets.fixed(ylims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5 # choose an index you want to plot\n",
    "hw1_helper.plot_data_generative(history=history, future=future, prediction=prediction, index=index, xlims=xlims, ylims=ylims)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
